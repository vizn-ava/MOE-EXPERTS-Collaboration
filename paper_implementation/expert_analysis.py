import json
import torch
import numpy as np
import pandas as pd
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import chi2_contingency
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

class MoEExpertAnalyzer:
    def __init__(self, metadata_path, activations_path):
        """åˆå§‹åŒ–MoEä¸“å®¶åˆ†æå™¨"""
        print("Loading data...")
        
        # åŠ è½½æ•°æ®
        with open(metadata_path, 'r', encoding='utf-8') as f:
            self.token_metadata = json.load(f)
        
        self.token_activations = torch.load(activations_path)
        
        # åŸºæœ¬ä¿¡æ¯
        self.num_tokens = len(self.token_metadata)
        self.num_experts = self.token_activations.shape[1]
        
        print(f"Loaded {self.num_tokens} tokens with {self.num_experts} experts")
        
        # æå–é¢†åŸŸä¿¡æ¯
        self.domains = [item['domain'] for item in self.token_metadata]
        self.unique_domains = list(set(self.domains))
        
        print(f"Found {len(self.unique_domains)} domains: {self.unique_domains}")
    
    def build_expert_token_mapping(self, activation_threshold=0.1, sample_ratio=1.0, random_seed=42):
        """å»ºç«‹ä¸“å®¶ä¸tokençš„æ˜ å°„å…³ç³»"""
        print(f"\nBuilding expert-token mapping (threshold: {activation_threshold}, sample_ratio: {sample_ratio}, random_seed: {random_seed})...")
        
        # åˆ›å»ºä¸“å®¶-tokenæ˜ å°„
        expert_token_map = defaultdict(list)
        token_expert_map = defaultdict(list)
        
        # è®¡ç®—è¦å¤„ç†çš„tokenæ•°é‡
        num_tokens_to_process = int(self.num_tokens * sample_ratio)
        print(f"Processing {num_tokens_to_process} out of {self.num_tokens} tokens ({sample_ratio*100:.1f}%)")
        
        # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
        np.random.seed(random_seed)
        
        # éšæœºé€‰æ‹©è¦å¤„ç†çš„tokenç´¢å¼•
        if sample_ratio >= 1.0:
            # å¦‚æœå¤„ç†å…¨éƒ¨tokenï¼Œåˆ™æŒ‰é¡ºåºå¤„ç†
            token_indices = list(range(self.num_tokens))
        else:
            # éšæœºé‡‡æ ·æŒ‡å®šæ•°é‡çš„token
            token_indices = np.random.choice(self.num_tokens, size=num_tokens_to_process, replace=False)
            token_indices = sorted(token_indices)  # æ’åºä»¥ä¾¿è¿›åº¦æ¡æ˜¾ç¤ºæ›´ç›´è§‚
        
        print(f"Sampling method: {'Sequential (full dataset)' if sample_ratio >= 1.0 else 'Random sampling'}")
        
        # éå†é€‰å®šçš„tokenï¼Œæ·»åŠ è¿›åº¦æ¡
        for token_idx in tqdm(token_indices, desc="Processing tokens", unit="token"):
            activations = self.token_activations[token_idx]
            active_experts = torch.where(activations > activation_threshold)[0].tolist()
            
            # è®°å½•æ˜ å°„å…³ç³»
            for expert_idx in active_experts:
                expert_token_map[expert_idx].append({
                    'token_idx': token_idx,
                    'activation': activations[expert_idx].item(),
                    'domain': self.domains[token_idx],
                    'token_text': self.token_metadata[token_idx]['token_text']
                })
                
                token_expert_map[token_idx].append({
                    'expert_idx': expert_idx,
                    'activation': activations[expert_idx].item()
                })
        
        self.expert_token_map = expert_token_map
        self.token_expert_map = token_expert_map
        
        print(f"Mapping completed. Active experts: {len(expert_token_map)}")
        return expert_token_map, token_expert_map
    
    def analyze_domain_preferences(self):
        """åˆ†æä¸“å®¶å¯¹ä¸åŒé¢†åŸŸçš„åå¥½"""
        print("\nAnalyzing domain preferences...")
        
        # è®¡ç®—æ¯ä¸ªä¸“å®¶åœ¨å„é¢†åŸŸçš„æ¿€æ´»ç»Ÿè®¡
        expert_domain_stats = {}
        
        # æ·»åŠ è¿›åº¦æ¡æ˜¾ç¤ºä¸“å®¶åˆ†æè¿›åº¦
        for expert_idx, token_list in tqdm(self.expert_token_map.items(), desc="Analyzing experts", unit="expert"):
            domain_activations = defaultdict(list)
            
            for token_info in token_list:
                domain = token_info['domain']
                activation = token_info['activation']
                domain_activations[domain].append(activation)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            stats = {}
            for domain in self.unique_domains:
                if domain in domain_activations:
                    activations = domain_activations[domain]
                    stats[domain] = {
                        'count': len(activations),
                        'mean_activation': np.mean(activations),
                        'max_activation': np.max(activations),
                        'total_activation': np.sum(activations)
                    }
                else:
                    stats[domain] = {
                        'count': 0,
                        'mean_activation': 0.0,
                        'max_activation': 0.0,
                        'total_activation': 0.0
                    }
            
            expert_domain_stats[expert_idx] = stats
        
        self.expert_domain_stats = expert_domain_stats
        return expert_domain_stats
    
    def calculate_expert_focus_degree(self):
        """è®¡ç®—ä¸“å®¶çš„ä¸“æ³¨ç¨‹åº¦ï¼šå¹³å‡æ¥çœ‹ä¸“å®¶æœ€ä¸“æ³¨é¢†åŸŸçš„ç™¾åˆ†æ¯”"""
        print("\nCalculating expert focus degree...")
        
        if not hasattr(self, 'expert_domain_stats'):
            print("âŒ è¯·å…ˆè¿è¡Œ analyze_domain_preferences() æ–¹æ³•")
            return None
        
        focus_degrees = []
        expert_focus_details = {}
        
        # æ·»åŠ è¿›åº¦æ¡æ˜¾ç¤ºä¸“æ³¨ç¨‹åº¦è®¡ç®—è¿›åº¦
        for expert_idx, domain_stats in tqdm(self.expert_domain_stats.items(), desc="Calculating focus degree", unit="expert"):
            # è®¡ç®—æ¯ä¸ªé¢†åŸŸçš„æ€»æ¿€æ´»é‡
            domain_totals = {}
            total_activation = 0
            
            for domain, stats in domain_stats.items():
                domain_total = stats['total_activation']
                domain_totals[domain] = domain_total
                total_activation += domain_total
            
            if total_activation > 0:
                # è®¡ç®—æ¯ä¸ªé¢†åŸŸçš„ç™¾åˆ†æ¯”
                domain_percentages = {domain: (total / total_activation) * 100 
                                    for domain, total in domain_totals.items()}
                
                # æ‰¾åˆ°æœ€ä¸“æ³¨çš„é¢†åŸŸåŠå…¶ç™¾åˆ†æ¯”
                max_domain = max(domain_percentages, key=domain_percentages.get)
                max_percentage = domain_percentages[max_domain]
                
                focus_degrees.append(max_percentage)
                expert_focus_details[expert_idx] = {
                    'most_focused_domain': max_domain,
                    'focus_percentage': max_percentage,
                    'domain_percentages': domain_percentages
                }
            else:
                # å¦‚æœä¸“å®¶æ²¡æœ‰æ¿€æ´»ï¼Œè·³è¿‡
                expert_focus_details[expert_idx] = {
                    'most_focused_domain': 'None',
                    'focus_percentage': 0.0,
                    'domain_percentages': {}
                }
        
        # è®¡ç®—å¹³å‡ä¸“æ³¨ç¨‹åº¦
        if focus_degrees:
            average_focus_degree = np.mean(focus_degrees)
            median_focus_degree = np.median(focus_degrees)
            std_focus_degree = np.std(focus_degrees)
            
            print(f"\nğŸ“Š ä¸“å®¶ä¸“æ³¨ç¨‹åº¦åˆ†æç»“æœ:")
            print(f"   ğŸ¯ å¹³å‡ä¸“æ³¨ç¨‹åº¦: {average_focus_degree:.2f}%")
            print(f"   ğŸ“ˆ ä¸­ä½æ•°ä¸“æ³¨ç¨‹åº¦: {median_focus_degree:.2f}%")
            print(f"   ğŸ“Š æ ‡å‡†å·®: {std_focus_degree:.2f}%")
            print(f"   ğŸ”¢ åˆ†æçš„ä¸“å®¶æ•°é‡: {len(focus_degrees)}")
            
            # æ˜¾ç¤ºä¸“æ³¨ç¨‹åº¦åˆ†å¸ƒ
            print(f"\nğŸ“ˆ ä¸“æ³¨ç¨‹åº¦åˆ†å¸ƒ:")
            bins = [0, 20, 40, 60, 80, 100]
            bin_labels = ['0-20%', '20-40%', '40-60%', '60-80%', '80-100%']
            hist, _ = np.histogram(focus_degrees, bins=bins)
            
            for i, (label, count) in enumerate(zip(bin_labels, hist)):
                percentage = (count / len(focus_degrees)) * 100
                print(f"   {label}: {count} ä¸ªä¸“å®¶ ({percentage:.1f}%)")
            
            # æ‰¾å‡ºæœ€ä¸“æ³¨å’Œæœ€ä¸ä¸“æ³¨çš„ä¸“å®¶
            if expert_focus_details:
                sorted_experts = sorted(expert_focus_details.items(), 
                                      key=lambda x: x[1]['focus_percentage'], reverse=True)
                
                print(f"\nğŸ† æœ€ä¸“æ³¨çš„5ä¸ªä¸“å®¶:")
                for i, (expert_idx, details) in enumerate(sorted_experts[:5]):
                    print(f"   {i+1}. ä¸“å®¶ {expert_idx}: {details['focus_percentage']:.2f}% "
                          f"(ä¸“æ³¨äº {details['most_focused_domain']})")
                
                print(f"\nğŸ” æœ€ä¸ä¸“æ³¨çš„5ä¸ªä¸“å®¶:")
                for i, (expert_idx, details) in enumerate(sorted_experts[-5:]):
                    print(f"   {i+1}. ä¸“å®¶ {expert_idx}: {details['focus_percentage']:.2f}% "
                          f"(ä¸“æ³¨äº {details['most_focused_domain']})")
            
            self.expert_focus_details = expert_focus_details
            self.average_focus_degree = average_focus_degree
            
            # ç”Ÿæˆä¸“æ³¨ç¨‹åº¦åˆ†å¸ƒå›¾
            self.plot_focus_degree_distribution(focus_degrees)
            
            return {
                 'average_focus_degree': average_focus_degree,
                 'median_focus_degree': median_focus_degree,
                 'std_focus_degree': std_focus_degree,
                 'focus_degrees': focus_degrees,
                 'expert_focus_details': expert_focus_details
             }
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä¸“å®¶æ¿€æ´»æ•°æ®")
            return None
    
    def plot_focus_degree_distribution(self, focus_degrees):
        """Plot expert focus degree distribution"""
        print("\nğŸ“Š Generating focus degree distribution chart...")
        
        plt.figure(figsize=(12, 8))
        
        # Create subplots
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Expert Focus Degree Analysis', fontsize=16, fontweight='bold')
        
        # 1. Histogram
        ax1.hist(focus_degrees, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        ax1.set_xlabel('Focus Degree (%)')
        ax1.set_ylabel('Number of Experts')
        ax1.set_title('Focus Degree Distribution Histogram')
        ax1.grid(True, alpha=0.3)
        
        # Add statistical information
        mean_val = np.mean(focus_degrees)
        median_val = np.median(focus_degrees)
        ax1.axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.2f}%')
        ax1.axvline(median_val, color='orange', linestyle='--', label=f'Median: {median_val:.2f}%')
        ax1.legend()
        
        # 2. Box plot
        ax2.boxplot(focus_degrees, vert=True, patch_artist=True, 
                   boxprops=dict(facecolor='lightgreen', alpha=0.7))
        ax2.set_ylabel('Focus Degree (%)')
        ax2.set_title('Focus Degree Box Plot')
        ax2.grid(True, alpha=0.3)
        
        # 3. Grouped bar chart
        bins = [0, 20, 40, 60, 80, 100]
        bin_labels = ['0-20%', '20-40%', '40-60%', '60-80%', '80-100%']
        hist, _ = np.histogram(focus_degrees, bins=bins)
        
        colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc']
        bars = ax3.bar(bin_labels, hist, color=colors, alpha=0.7, edgecolor='black')
        ax3.set_xlabel('Focus Degree Range')
        ax3.set_ylabel('Number of Experts')
        ax3.set_title('Focus Degree Grouped Distribution')
        ax3.grid(True, alpha=0.3)
        
        # Add value labels on bars
        for bar, count in zip(bars, hist):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 5,
                    f'{count}\n({count/len(focus_degrees)*100:.1f}%)',
                    ha='center', va='bottom', fontsize=9)
        
        # 4. Cumulative distribution plot
        sorted_degrees = np.sort(focus_degrees)
        cumulative = np.arange(1, len(sorted_degrees) + 1) / len(sorted_degrees) * 100
        ax4.plot(sorted_degrees, cumulative, linewidth=2, color='purple')
        ax4.set_xlabel('Focus Degree (%)')
        ax4.set_ylabel('Cumulative Percentage (%)')
        ax4.set_title('Focus Degree Cumulative Distribution')
        ax4.grid(True, alpha=0.3)
        
        # Add key percentile lines
        percentiles = [25, 50, 75]
        for p in percentiles:
            val = np.percentile(focus_degrees, p)
            ax4.axvline(val, color='red', linestyle=':', alpha=0.7, 
                       label=f'{p}th: {val:.1f}%')
        ax4.legend()
        
        plt.tight_layout()
        
        # Save image
        output_path = 'expert_focus_degree_distribution.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"âœ… Focus degree distribution chart saved to: {output_path}")
        
        plt.close()  # Close figure to free memory
    
    def find_domain_specialized_experts(self, top_k=10):
        """æ‰¾å‡ºå¯¹ç‰¹å®šé¢†åŸŸç‰¹åŒ–çš„ä¸“å®¶"""
        print(f"\nFinding top {top_k} domain-specialized experts...")
        
        domain_specialists = {}
        
        for domain in self.unique_domains:
            # è®¡ç®—æ¯ä¸ªä¸“å®¶åœ¨è¯¥é¢†åŸŸçš„ä¸“ä¸šåŒ–ç¨‹åº¦
            expert_scores = []
            
            for expert_idx, stats in self.expert_domain_stats.items():
                domain_stat = stats[domain]
                total_activations = sum(s['total_activation'] for s in stats.values())
                
                if total_activations > 0:
                    # ä¸“ä¸šåŒ–åˆ†æ•°ï¼šè¯¥é¢†åŸŸæ¿€æ´»å æ¯” Ã— å¹³å‡æ¿€æ´»å¼ºåº¦
                    specialization_score = (domain_stat['total_activation'] / total_activations) * domain_stat['mean_activation']
                    expert_scores.append((expert_idx, specialization_score, domain_stat))
            
            # æ’åºå¹¶å–å‰kä¸ª
            expert_scores.sort(key=lambda x: x[1], reverse=True)
            domain_specialists[domain] = expert_scores[:top_k]
        
        self.domain_specialists = domain_specialists
        return domain_specialists
    
    def create_domain_expert_matrix(self):
        """åˆ›å»ºé¢†åŸŸ-ä¸“å®¶æ¿€æ´»çŸ©é˜µ"""
        print("\nCreating domain-expert activation matrix...")
        
        # åˆ›å»ºçŸ©é˜µï¼šè¡Œä¸ºé¢†åŸŸï¼Œåˆ—ä¸ºä¸“å®¶
        matrix = np.zeros((len(self.unique_domains), self.num_experts))
        
        for domain_idx, domain in enumerate(self.unique_domains):
            for expert_idx in range(self.num_experts):
                if expert_idx in self.expert_domain_stats:
                    stats = self.expert_domain_stats[expert_idx][domain]
                    matrix[domain_idx, expert_idx] = stats['mean_activation']
        
        self.domain_expert_matrix = matrix
        return matrix
    
    def generate_summary_report(self):
        """ç”Ÿæˆåˆ†ææ‘˜è¦æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("MoE EXPERT ANALYSIS SUMMARY REPORT")
        print("="*60)
        
        print(f"\nğŸ“Š Data Overview:")
        print(f"  â€¢ Total tokens: {self.num_tokens:,}")
        print(f"  â€¢ Total experts: {self.num_experts:,}")
        print(f"  â€¢ Domains: {len(self.unique_domains)}")
        
        # é¢†åŸŸåˆ†å¸ƒ
        domain_counts = Counter(self.domains)
        print(f"\nğŸ·ï¸  Domain Distribution:")
        for domain, count in domain_counts.most_common():
            percentage = (count / self.num_tokens) * 100
            print(f"  â€¢ {domain}: {count:,} tokens ({percentage:.1f}%)")
        
        # ä¸“å®¶æ¿€æ´»ç»Ÿè®¡
        active_experts = len(self.expert_token_map)
        print(f"\nğŸ§  Expert Activation:")
        print(f"  â€¢ Active experts: {active_experts:,} / {self.num_experts:,} ({active_experts/self.num_experts*100:.1f}%)")
        
        # é¢†åŸŸä¸“å®¶åˆ†æ
        print(f"\nğŸ¯ Domain Specialists (Top 3 per domain):")
        for domain, specialists in self.domain_specialists.items():
            print(f"\n  {domain}:")
            for i, (expert_idx, score, stats) in enumerate(specialists[:3]):
                print(f"    {i+1}. Expert {expert_idx}: score={score:.4f}, "
                      f"activations={stats['count']}, avg_strength={stats['mean_activation']:.4f}")
        
        return {
            'total_tokens': self.num_tokens,
            'total_experts': self.num_experts,
            'active_experts': active_experts,
            'domain_distribution': dict(domain_counts),
            'domain_specialists': self.domain_specialists
        }

def main():
    # æ–‡ä»¶è·¯å¾„
    metadata_path = 'E:/tyb_file/tyb_tasks/research/MoE_vis/output/output/token_metadata.json'
    activations_path = 'E:/tyb_file/tyb_tasks/research/MoE_vis/output/output/R_tensor0925.pt'
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = MoEExpertAnalyzer(metadata_path, activations_path)
    
    # æ‰§è¡Œåˆ†æ
    analyzer.build_expert_token_mapping(activation_threshold=0.1, sample_ratio=1/3, random_seed=42)
    analyzer.analyze_domain_preferences()
    analyzer.calculate_expert_focus_degree()  # æ·»åŠ ä¸“æ³¨ç¨‹åº¦åˆ†æ
    analyzer.find_domain_specialized_experts(top_k=10)
    analyzer.create_domain_expert_matrix()
    
    # ç”ŸæˆæŠ¥å‘Š
    summary = analyzer.generate_summary_report()
    
    # ä¿å­˜ç»“æœ
    results = {
        'expert_token_mapping': {k: v for k, v in list(analyzer.expert_token_map.items())[:100]},  # ä¿å­˜å‰100ä¸ªä¸“å®¶çš„æ˜ å°„
        'domain_specialists': analyzer.domain_specialists,
        'summary': summary
    }
    
    with open('expert_analysis_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… Analysis completed! Results saved to 'expert_analysis_results.json'")
    
    return analyzer

if __name__ == "__main__":
    analyzer = main()